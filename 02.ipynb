{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8233d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 영화 리뷰 수: 2000\n",
      "카테고리: ['neg', 'pos']\n",
      "부정 리뷰: 1000개\n",
      "긍정 리뷰: 1000개\n",
      "\n",
      "첫 번째 리뷰 ID: neg/cv000_29416.txt\n",
      "원문 일부:\n",
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "w\n",
      "\n",
      "문장 토큰화 (첫 2개):\n",
      "  1: ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.']\n",
      "  2: ['they', 'get', 'into', 'an', 'accident', '.']\n",
      "\n",
      "단어 토큰화 (첫 20개): ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\playdata2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\playdata2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1단계: 필요한 데이터 다운로드\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 2단계: 데이터 탐색\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# 데이터셋 크기 파악\n",
    "print(f\"전체 영화 리뷰 수: {len(movie_reviews.fileids())}\")\n",
    "print(f\"카테고리: {movie_reviews.categories()}\")  # ['neg', 'pos']\n",
    "print(f\"부정 리뷰: {len(movie_reviews.fileids(categories='neg'))}개\")\n",
    "print(f\"긍정 리뷰: {len(movie_reviews.fileids(categories='pos'))}개\")\n",
    "\n",
    "# 3단계: 첫 번째 리뷰 살펴보기\n",
    "first_review_id = movie_reviews.fileids()[0]\n",
    "first_review = movie_reviews.raw(first_review_id)\n",
    "print(f\"\\n첫 번째 리뷰 ID: {first_review_id}\")\n",
    "print(f\"원문 일부:\\n{first_review[:200]}\")\n",
    "\n",
    "# 4단계: 토큰화 결과 확인\n",
    "sentences = movie_reviews.sents(first_review_id)  # 문장 단위 토큰화\n",
    "words = movie_reviews.words(first_review_id)      # 단어 단위 토큰화\n",
    "\n",
    "print(f\"\\n문장 토큰화 (첫 2개):\")\n",
    "for i, sent in enumerate(sentences[:2]):\n",
    "    print(f\"  {i+1}: {sent}\")\n",
    "\n",
    "print(f\"\\n단어 토큰화 (첫 20개): {words[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc06b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegexpTokenizer: 정규표현식으로 정확한 토큰화\n",
    "# stopwords : 문법적 기능을 제거하고 단어에 집중\n",
    "# 상위 N개 단어 선택 : 메모리효율성과 노이즈 제거의 균형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0db137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문서 수: 2000\n",
      "첫 문서의 단어 수: 879\n",
      "첫 문서의 첫 50개 단어:\n",
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch']\n",
      "\n",
      "상위 10개 빈도 단어:\n",
      "  1. ',': 77717회\n",
      "  2. 'the': 76529회\n",
      "  3. '.': 65876회\n",
      "  4. 'a': 38106회\n",
      "  5. 'and': 35576회\n",
      "  6. 'of': 34123회\n",
      "  7. 'to': 31937회\n",
      "  8. ''': 30585회\n",
      "  9. 'is': 25195회\n",
      "  10. 'in': 21822회\n",
      "\n",
      "전체 서로 다른 단어 수: 43011\n",
      "\n",
      "처리 후 상위 10개 단어:\n",
      "  1. 'film': 8935회\n",
      "  2. 'one': 5791회\n",
      "  3. 'movie': 5538회\n",
      "  4. 'like': 3690회\n",
      "  5. 'even': 2564회\n",
      "  6. 'time': 2409회\n",
      "  7. 'good': 2407회\n",
      "  8. 'story': 2136회\n",
      "  9. 'would': 2084회\n",
      "  10. 'much': 2049회\n",
      "\n",
      "특성으로 선택된 단어 수: 1000\n",
      "특성 예시: ['film', 'one', 'movie', 'like', 'even', 'time', 'good', 'story', 'would', 'much', 'also', 'get', 'character', 'two', 'well', 'first', 'characters', 'see', 'way', 'make']\n"
     ]
    }
   ],
   "source": [
    "# BOW - 수동으로 벡터 생성\n",
    "# 1단계: 모든 문서를 단어 리스트로 변환\n",
    "documents = [list(movie_reviews.words(fileid)) \n",
    "             for fileid in movie_reviews.fileids()]\n",
    "\n",
    "print(f\"전체 문서 수: {len(documents)}\")\n",
    "print(f\"첫 문서의 단어 수: {len(documents[0])}\")\n",
    "print(f\"첫 문서의 첫 50개 단어:\\n{documents[0][:50]}\")\n",
    "\n",
    "# 2단계: 전체 단어 빈도 계산 (불용어 제외 전)\n",
    "word_count = {}\n",
    "for doc in documents:\n",
    "    for word in doc:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "\n",
    "# 상위 10개 빈도 단어 확인\n",
    "sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\n상위 10개 빈도 단어:\")\n",
    "for i, (word, count) in enumerate(sorted_words[:10], 1):\n",
    "    print(f\"  {i}. '{word}': {count}회\")\n",
    "\n",
    "# 3단계: 불용어 제거 후 처리\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 정규표현식으로 3글자 이상의 단어만 추출\n",
    "tokenizer = RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "# 영어 불용어 로드\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "# 모든 리뷰를 토큰화하고 불용어 제거\n",
    "processed_documents = []\n",
    "for fileid in movie_reviews.fileids():\n",
    "    raw_text = movie_reviews.raw(fileid)\n",
    "    tokens = [token for token in tokenizer.tokenize(raw_text) \n",
    "              if token not in english_stops]\n",
    "    processed_documents.append(tokens)\n",
    "\n",
    "# 처리 후 단어 빈도 재계산\n",
    "word_count_processed = {}\n",
    "for doc in processed_documents:\n",
    "    for word in doc:\n",
    "        word_count_processed[word] = word_count_processed.get(word, 0) + 1\n",
    "\n",
    "sorted_processed = sorted(word_count_processed.items(), \n",
    "                         key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n전체 서로 다른 단어 수: {len(sorted_processed)}\")\n",
    "print(\"\\n처리 후 상위 10개 단어:\")\n",
    "for i, (word, count) in enumerate(sorted_processed[:10], 1):\n",
    "    print(f\"  {i}. '{word}': {count}회\")\n",
    "\n",
    "# 4단계: 특성 선택 (상위 1000개 단어)\n",
    "word_features = [word for word, count in sorted_processed[:1000]]\n",
    "print(f\"\\n특성으로 선택된 단어 수: {len(word_features)}\")\n",
    "print(f\"특성 예시: {word_features[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b3dab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323 130 262 278 391 "
     ]
    }
   ],
   "source": [
    "# processed_documents[0]  # 문장을 토큰화(3개의 연속된 문장, 불용어제거)\n",
    "for doc in processed_documents[:5]:\n",
    "    print(len(doc), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e53892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', 'two', 'teen']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_documents[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c3dd948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 단어 리스트: ['one', 'two', 'teen', 'couples', 'solo']\n",
      "테스트 문서: ['two', 'two', 'couples']\n",
      "결과 벡터: [0, 2, 0, 1, 0]\n",
      "→ 'two'가 2번, 'couples'가 1번, 나머지는 0\n",
      "\n",
      "생성된 특성 벡터 수: 2000\n",
      "각 벡터의 차원: 1000\n",
      "\n",
      "첫 문서 벡터 (처음 20개):\n",
      "  'film': 5\n",
      "  'one': 3\n",
      "  'movie': 6\n",
      "  'like': 3\n",
      "  'even': 3\n",
      "  'time': 0\n",
      "  'good': 2\n",
      "  'story': 0\n",
      "  'would': 1\n",
      "  'much': 0\n",
      "  'also': 1\n",
      "  'get': 3\n",
      "  'character': 1\n",
      "  'two': 2\n",
      "  'well': 1\n",
      "  'first': 0\n",
      "  'characters': 1\n",
      "  'see': 2\n",
      "  'way': 3\n",
      "  'make': 5\n"
     ]
    }
   ],
   "source": [
    "# 각 문서의 고정된 길이의 벡터로 변환(모든 문서가 같은 차원 )\n",
    "# 기계학습 알고리즘의 입력 형식으로 변환\n",
    "def document_features(document, word_features):\n",
    "    \"\"\"\n",
    "    문서를 특성 벡터로 변환\n",
    "    \n",
    "    Args:\n",
    "        document: 토큰화된 단어 리스트\n",
    "        word_features: 특성으로 사용할 단어 리스트\n",
    "    \n",
    "    Returns:\n",
    "        document의 각 특성에 대한 빈도 리스트\n",
    "    \"\"\"\n",
    "    # 문서 내 단어 빈도 계산\n",
    "    word_count = {}\n",
    "    for word in document:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "    \n",
    "    # 특성 벡터 생성\n",
    "    features = []\n",
    "    for word in word_features:\n",
    "        # 특성 단어가 문서에 없으면 0\n",
    "        features.append(word_count.get(word, 0))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# 테스트 실행\n",
    "test_features = ['one', 'two', 'teen', 'couples', 'solo']\n",
    "test_doc = ['two', 'two', 'couples']\n",
    "result = document_features(test_doc, test_features)\n",
    "\n",
    "print(\"테스트 단어 리스트:\", test_features)\n",
    "print(\"테스트 문서:\", test_doc)\n",
    "print(\"결과 벡터:\", result)\n",
    "print(\"→ 'two'가 2번, 'couples'가 1번, 나머지는 0\")\n",
    "\n",
    "# 모든 문서에 대해 특성 벡터 생성\n",
    "feature_sets = [document_features(doc, word_features) \n",
    "                 for doc in processed_documents]\n",
    "\n",
    "print(f\"\\n생성된 특성 벡터 수: {len(feature_sets)}\")\n",
    "print(f\"각 벡터의 차원: {len(feature_sets[0])}\")\n",
    "print(f\"\\n첫 문서 벡터 (처음 20개):\")\n",
    "for i, (word, count) in enumerate(zip(word_features[:20], feature_sets[0][:20])):\n",
    "    print(f\"  '{word}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5889115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['film', 'one', 'movie'], [5, 3, 6], 1000, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features[:3],  feature_sets[0][:3],  len(word_features), len(feature_sets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64beacb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특성수 : ['film' 'one' 'movie' 'like' 'even']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 데이터 준비\n",
    "reivews = [  movie_reviews.raw(fileid) for fileid in movie_reviews.fileids() ]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(vocabulary=word_features)\n",
    "reviews_cv = cv.fit_transform(reivews)\n",
    "print(f'특성수 : {cv.get_feature_names_out()[:5]}')\n",
    "reviews_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40a9006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/file/d/1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o/view?usp=sharing\n",
    "\n",
    "import pandas as pd\n",
    "url = \"https://drive.google.com/uc?id=1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0d68532b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>돈 들인건 티가 나지만 보는 내내 하품만</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.10.29</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.26</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.24</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이 정도면 볼만하다고 할 수 있음!</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.22</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>재미있다</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.20</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating        date  \\\n",
       "0                             돈 들인건 티가 나지만 보는 내내 하품만       1  2018.10.29   \n",
       "1       몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.      10  2018.10.26   \n",
       "2  이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...       8  2018.10.24   \n",
       "3                                이 정도면 볼만하다고 할 수 있음!       8  2018.10.22   \n",
       "4                                               재미있다      10  2018.10.20   \n",
       "\n",
       "    title  \n",
       "0  인피니티 워  \n",
       "1  인피니티 워  \n",
       "2  인피니티 워  \n",
       "3  인피니티 워  \n",
       "4  인피니티 워  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f3d149c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\LLM\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 리뷰 (처음 200자):\n",
      "돈 들인건 티가 나지만 보는 내내 하품만\n",
      "\n",
      "쿼리 텍스트 (처음 150자):\n",
      "돈 들인건 티가 나지만 보는 내내 하품만\n",
      "\n",
      "쿼리 벡터 크기: (1, 1000)\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "url = \"https://drive.google.com/uc?id=1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o\"\n",
    "df = pd.read_csv(url)\n",
    "okt = Okt()\n",
    "def custom_tokenizer(doc):\n",
    "    \"\"\"\n",
    "    형태소 분석 후 명사, 동사, 형용사만 추출\n",
    "    \"\"\"\n",
    "    pos_tags = okt.pos(doc)\n",
    "    tokens = [word for word, pos in pos_tags \n",
    "              if pos in ['Noun', 'Verb', 'Adjective']]\n",
    "    return tokens\n",
    "\n",
    "daum_cv = CountVectorizer(\n",
    "    max_features=1000,\n",
    "    tokenizer=custom_tokenizer\n",
    ")\n",
    "\n",
    "reviews = df.review\n",
    "daum_dtm  = daum_cv.fit_transform(reviews)\n",
    "original_review = reviews[0]  # 첫 번째 리뷰\n",
    "print(f\"원본 리뷰 (처음 200자):\\n{original_review[:200]}\\n\")\n",
    "\n",
    "# 문서의 뒤 절반을 query로 사용 (부분 검색 시나리오)\n",
    "midpoint = len(original_review) // 2\n",
    "# query_text = original_review[midpoint:]  # 뒤 절반\n",
    "query_text = original_review\n",
    "print(f\"쿼리 텍스트 (처음 150자):\\n{query_text[:150]}\\n\")\n",
    "\n",
    "# 2단계: 쿼리 문서를 벡터로 변환\n",
    "query_vector = daum_cv.transform([query_text])\n",
    "print(f\"쿼리 벡터 크기: {query_vector.shape}\")\n",
    "\n",
    "# 유사도 분석\n",
    "scores = cosine_similarity(query_vector,daum_dtm)\n",
    "most_simular_idx = np.argmax(scores)\n",
    "# scores[most_simular_idx], reviews[most_simular_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "40cdb5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'돈 들인건 티가 나지만 보는 내내 하품만'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "50801b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['돈 들인건 티가 나지만 보는 내내 하품만', '여전한 군바리 국가지배...보는 내내 슬펐다.',\n",
       "       '보는 내내 설레였다', '보는 내내 눈시울이 ㅠㅠ', '보는 내내 너무 괴로웠다'], dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(reviews)[scores[0].argsort()[::-1][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "13a0ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개선 TF-IDF\n",
    "# 단어의 상대적 중요도를 반영한 벡터화 기법\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "# TF : 특정단어가 문서에서 얼마나 자주나타나는지 비율\n",
    "# 해당 단어의 빈도 / 문서의 전체 단어 수   좋다  문서에서 10번   해당문서는 100단어   10/100\n",
    "# IDF (Inverse Document Frequency)  단어가 전체 문장에서 얼마나 드문  (희귀)\n",
    "# log(전체문서 / 해당 단어 포함 문서) 단어의 가중치를 낮추기위해서 log적용\n",
    "# 2000개 문서 중 100개만 \"좋다\" log(2000/100) = 2.99\n",
    "# TF-IDF  TF x IDF  --> 특정 문서에서 의미 있는 단어에 높은 가중치 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f44f6d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\LLM\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((14725, 1000), (14725, 1000))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_cv = CountVectorizer(\n",
    "    max_features=1000,\n",
    "    tokenizer=custom_tokenizer\n",
    ")\n",
    "count_dtm = count_cv.fit_transform(reviews)  # 전체 문서를 Bow 벡터화\n",
    "tfidf_cv =  TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    tokenizer=custom_tokenizer\n",
    ")\n",
    "tfidf_dtm = tfidf_cv.fit_transform(reivews)  # 전체 문서를 TF-IDF 벡터화\n",
    "count_dtm.shape,  tfidf_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6e38fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리 벡터화\n",
    "query_count =  count_cv.transform([query_text])\n",
    "query_tfidf =  tfidf_cv.transform([query_text])\n",
    "# 코사인 유사도 계산\n",
    "count_sim = cosine_similarity(query_count,count_dtm)[0]\n",
    "tfidf_sim = cosine_similarity(query_tfidf,tfidf_dtm)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8ead7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 2, 1, 0])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "a.argsort()  # 오름차순으로 인덱스\n",
    "(-a).argsort()  # 값에 -를 붙이면.. 오름차순 인덱스는 내림차순과 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f04e63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_count_index = (-count_sim).argsort()[:5]\n",
    "top5_tfidf_index = (-tfidf_sim).argsort()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "29bf20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = np.array(reivews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b477758f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['보는 내내 너무 괴로웠다', '보는 내내 설레였다', '보는 내내 눈시울이 ㅠㅠ',\n",
       "       '여전한 군바리 국가지배...보는 내내 슬펐다.'], dtype='<U482')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[top5_count_index][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "845795d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['보는 내내 설레였다', '보는 내내 너무 괴로웠다', '보는 내내 눈시울이 ㅠㅠ',\n",
       "       '여전한 군바리 국가지배...보는 내내 슬펐다.'], dtype='<U482')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[top5_tfidf_index][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1ab0502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_reivew = '숙면을 하기 좋은 영화.. 강추..'\n",
    "# my_review  count 방식이나 또는 tf-idf 방식으로 벡터화 한후... 전체리뷰를 만약 tf-idf 방식이면 전체 리뷰를 tf-idf 벡터화 한\n",
    "# 전체데이터와 함께 유사도 방식으로 점수를 구해서 상위 N개의 문서를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "defe7172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['레이버 인포매틱스 공부하기 싫다', '강추 영화네요^^', '강추', '강추!!', '강추!'],\n",
       "      dtype='<U482')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 쿼리 벡터화\n",
    "query_count =  count_cv.transform([my_reivew])\n",
    "query_tfidf =  tfidf_cv.transform([my_reivew])\n",
    "# 코사인 유사도 계산\n",
    "count_sim = cosine_similarity(query_count,count_dtm)[0]\n",
    "tfidf_sim = cosine_similarity(query_tfidf,tfidf_dtm)[0]\n",
    "\n",
    "top5_count_index = (-count_sim).argsort()[:5]\n",
    "top5_tfidf_index = (-tfidf_sim).argsort()[:5]\n",
    "\n",
    "reviews[top5_tfidf_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a477822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram  연속된 N개의 단위\n",
    "\n",
    "# 영화가 정말 재미있다\n",
    "# unigram  1-gram\n",
    "# [영화] [가]  [정말] [재미있다] \n",
    "\n",
    "# Bigram(2-gram)\n",
    "# [영화 가] [가 정말] [정말 재미있다]\n",
    "\n",
    "# Trigram(3-gram)\n",
    "# [영화 가 정말] [가 정말 재미있다]\n",
    "\n",
    "# 특성\n",
    "# 문맥정보 포함   : 단서의 순서와 관계를 반영\n",
    "# 더 나은 분류   : 좋은 영화 나쁜영화 구분\n",
    "# 의미 보존  : 인접한 단어들의 의존성 보존\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4102ec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 channel Terms of Service accepted\n",
      "Channels:\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: c:\\Users\\playdata2\\miniconda3\\envs\\LLM\n",
      "\n",
      "  added / updated specs:\n",
      "    - seaborn\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    seaborn-0.13.2             |  py313haa95532_3         725 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         725 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  seaborn            pkgs/main/win-64::seaborn-0.13.2-py313haa95532_3 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages: ...working...\n",
      "seaborn-0.13.2       | 725 KB    |            |   0% \n",
      "seaborn-0.13.2       | 725 KB    | 2          |   2% \n",
      "seaborn-0.13.2       | 725 KB    | ########## | 100% \n",
      "seaborn-0.13.2       | 725 KB    | ########## | 100% \n",
      "seaborn-0.13.2       | 725 KB    | ########## | 100% \n",
      "                                                     \n",
      " done\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e6370b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from konlpy.tag import Okt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "594740b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14723</th>\n",
       "      <td>간만에 제대로 잘짜여진 각본의 영화를 봤네 여운이 아직도 남아~어른을 위한 애니~</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.01.12</td>\n",
       "      <td>코코</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14724</th>\n",
       "      <td>한국개봉을 눈빠지게 기다린 보람이있다 깨우치는게 많은 영화</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.01.12</td>\n",
       "      <td>코코</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating        date  \\\n",
       "14723  간만에 제대로 잘짜여진 각본의 영화를 봤네 여운이 아직도 남아~어른을 위한 애니~      10  2018.01.12   \n",
       "14724               한국개봉을 눈빠지게 기다린 보람이있다 깨우치는게 많은 영화      10  2018.01.12   \n",
       "\n",
       "      title  label  \n",
       "14723    코코      1  \n",
       "14724    코코      1  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = (df.rating >=7).astype(int)\n",
    "# df['label'] = pd.qcut(df.rating, q=2,labels=[0,1])\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1ef6b1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "원본 텍스트: '영화가 정말 재미있다'\n",
      "형태소 토큰: ['영화', '가', '정말', '재미있다']\n",
      "\n",
      "1-gram (Unigram):\n",
      "  총 4개: ['영화', '가', '정말', '재미있다']\n",
      "    1. [영화]\n",
      "    2. [가]\n",
      "    3. [정말]\n",
      "    4. [재미있다]\n",
      "\n",
      "2-gram (Bigram):\n",
      "  총 3개: ['영화 가', '가 정말', '정말 재미있다']\n",
      "    1. [영화 가]\n",
      "    2. [가 정말]\n",
      "    3. [정말 재미있다]\n",
      "\n",
      "3-gram (Trigram):\n",
      "  총 2개: ['영화 가 정말', '가 정말 재미있다']\n",
      "    1. [영화 가 정말]\n",
      "    2. [가 정말 재미있다]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def explain_ngrams(text, n_values=[1, 2, 3]):\n",
    "    \"\"\"N-gram을 이해하기 쉽게 설명\"\"\"\n",
    "    tokens = okt.morphs(text)\n",
    "    \n",
    "    print(f\"\\n원본 텍스트: '{text}'\")\n",
    "    print(f\"형태소 토큰: {tokens}\\n\")\n",
    "    \n",
    "    for n in n_values:\n",
    "        ngrams = [' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "        print(f\"{n}-gram ({['Unigram', 'Bigram', 'Trigram'][n-1]}):\")\n",
    "        print(f\"  총 {len(ngrams)}개: {ngrams}\")\n",
    "        if len(ngrams) <= 10:\n",
    "            for i, gram in enumerate(ngrams, 1):\n",
    "                print(f\"    {i}. [{gram}]\")\n",
    "        print()\n",
    "\n",
    "# 샘플 텍스트에 대한 N-gram 설명\n",
    "sample_review = \"영화가 정말 재미있다\"\n",
    "explain_ngrams(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a3054cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "차원 : (14725, 50)\n",
      "!, ,, ., .., ..., ?, cg, ~, 가, 감동, 것, 고, 과, 그, 너무, 눈물, 는, 다, 더, 도\n"
     ]
    }
   ],
   "source": [
    "# 벡터화   n-gram별 벡터화 및 특성 비교\n",
    "def tokenizer_morphs(text):\n",
    "    '''형태소 기반 토크나이져'''\n",
    "    return okt.morphs(text)\n",
    "\n",
    "# 1-gram\n",
    "vec_1gram = TfidfVectorizer(tokenizer=tokenizer_morphs, ngram_range=(1,1), max_features=50)\n",
    "X_1gram = vec_1gram.fit_transform(df.review)\n",
    "print(f'차원 : {X_1gram.shape}')\n",
    "features_1gram = vec_1gram.get_feature_names_out()[:20]\n",
    "print(f'{', '.join(features_1gram)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "873c8844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "차원 : (14725, 1000)\n",
      "가는 줄, 가는줄 모르고, 가슴 먹, 감동 영화, 감동 입니다, 감동 있고, 감동 재미, 강철 비, 같은 영화, 것 같다, 것 같아요, 공포 영화, 광주 민주화, 광주 시민, 괜찮은 영화\n"
     ]
    }
   ],
   "source": [
    "# 2-gram\n",
    "vec_2gram = TfidfVectorizer(tokenizer=custom_tokenizer, ngram_range=(1,2), max_features=1000)\n",
    "X_2gram = vec_2gram.fit_transform(df.review)\n",
    "print(f'차원 : {X_2gram.shape}')\n",
    "features_2gram = vec_2gram.get_feature_names_out()\n",
    "bigram = [f for f in features_2gram if len(f.split()) > 1][:15]\n",
    "print(f'{', '.join(bigram)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "01d8728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이분할 \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test =  train_test_split(df.review, df.label,test_size=0.2, stratify=df.label, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d22e40a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8281833616298812"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 gram 모델\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "vec_1 = TfidfVectorizer(tokenizer=custom_tokenizer, ngram_range=(1,1),max_features=1000)\n",
    "x_train_1 = vec_1.fit_transform(x_train)\n",
    "x_test_1 = vec_1.transform(x_test)\n",
    "\n",
    "clf_1 = LogisticRegression()\n",
    "clf_1.fit(x_train_1,y_train)\n",
    "clf_1.score(x_test_1,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9f8eeda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.51      0.62       831\n",
      "           1       0.83      0.95      0.89      2114\n",
      "\n",
      "    accuracy                           0.83      2945\n",
      "   macro avg       0.82      0.73      0.76      2945\n",
      "weighted avg       0.83      0.83      0.81      2945\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict_1 = clf_1.predict(x_test_1)\n",
    "print( classification_report(y_test, predict_1) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
