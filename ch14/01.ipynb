{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182d8fb0",
   "metadata": {},
   "source": [
    "### BERT 양쪽을 문맥을 이해\n",
    "    - 빈칸채우기 \n",
    "\n",
    "####\n",
    "    - 데이터\n",
    "    - Hugging Face Transformers Pipeline(간단한 api)    \n",
    "    - 토크나이져 ( 텍스트 -> 토큰 ->숫자 ID)\n",
    "    - 사전학습된 모델(BERT , GPT-2, DistilBERT 등)\n",
    "    - 예측 결과(감성분석, 텍스트 생성, 문장 유사도)\n",
    "    - 후처리 평가(확률 변환, 정확도 계산)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed13b59",
   "metadata": {},
   "source": [
    "```\n",
    "\"I went to the [MASK] to buy some milk\"\n",
    "BERT 예측 : [MASK] = \"store\" ( 앞뒤 문맥 buy, milk를 보고 추론)\n",
    "```\n",
    "\n",
    "### 사전학습 방법\n",
    "    - MLM(Masked Language Model) : 문장의 15% 단어를 MASK 처리한다음 예측\n",
    "    - NSP(Next Stentece Prediction) : 두 문장이 연결되는지 판단\n",
    "```\n",
    "사전학습(대규모 텍스트)\n",
    "[CLS] token 추가\n",
    "특정 태스크 레이블로 학습(감성분석, QA 등)\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8658b8b3",
   "metadata": {},
   "source": [
    "### DistilBERT\n",
    "    - BERT 압축버전 : 속도는 2배 빠르다, 성능은 95% 유지함\n",
    "    - 두꺼운 교과서(BERT)의 핵심만 추린 요약본(DistilBERT)\n",
    "### 지식증류(Knowledge Distillation)\n",
    "    - Teacher 모델(BERT) : 소프트레이블 생성(확률분포)\n",
    "    - Student 모델(DistilBERT) : Teacher 출력을 모방\n",
    "### GPT-2\n",
    "    - 이전단어들을 보고 다음 단어를 예측    \n",
    "    - 단방향 Attention:\n",
    "    - Zero-shot Learning : 특정 태스트 학습 없이도 수행 가능\n",
    "    - GPT 시리즈\n",
    "        - GPT -1 : 117M 파라메터\n",
    "        - GPT -2 : 1.5B 파라메터\n",
    "        - GPT -3 : 175B 파라메터(Few-shot Learning)\n",
    "        - GPT -4 : 멀티모달\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545a9d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits : [2.5 1.  0.5]\n",
      "softmax : [0.73612472 0.16425163 0.09962365]\n",
      "softmax sum : 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def softmax(logits):\n",
    "    '''\n",
    "    로짓을 확률 분포로 변환\n",
    "    Args:\n",
    "        logits : 모델의 출력 점수( [2.5,1.0,0.5])\n",
    "    returns:\n",
    "        확률 분포(합이 1인 배열)\n",
    "    '''\n",
    "    # 수치 안정성을 위해 최대값을 빼줌(오버플로우 방지)\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / np.sum(exp_logits)\n",
    "logits = np.array([2.5,1.0,0.5])    \n",
    "probs = softmax(logits)\n",
    "print(f'logits : {logits}')\n",
    "print(f'softmax : {probs}')\n",
    "print(f'softmax sum : {np.sum(probs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b16c7",
   "metadata": {},
   "source": [
    "### WordPiece   Word + Piece\n",
    "    - 단어를 자주 등장하는 조각(piece) 단위로 잘라서 처리\n",
    "    - 기존토크나이져 대비 --> 더 잘게 쪼개자. (Subword)\n",
    "    - playing ----->  play + ##ing    ## 앞 조각에 붙는 서브워드 라는 의미\n",
    "    - 득템   ---------> 득 + 템"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485f8c4",
   "metadata": {},
   "source": [
    "WordPiece 토큰화 시뮬레이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b5e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb36ac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\LLM311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] i love natural language processing! [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# BERT 토크나이져 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# 테스트 문장\n",
    "sentence = 'I love natural language processing!'\n",
    "# 토큰화\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "token_ids = tokenizer.encode(sentence)\n",
    "# 디코딩\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(f'원본문장 : {}')\n",
    "print(f'토큰목록 : {}')\n",
    "print(f'토큰ID : {}')\n",
    "print(f'디코딩결과 : {}')\n",
    "\n",
    "print(f'CSL토큰 : {tokenizer.cls_token} -> {tokenizer.cls_token_id}')\n",
    "print(f'SEP토큰 : {tokenizer.sep_token} -> {tokenizer.sep_token_id}')\n",
    "print(f'PAD토큰 : {tokenizer.pad_token} -> {tokenizer.pad_token_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a3da2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
