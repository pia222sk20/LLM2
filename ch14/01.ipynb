{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182d8fb0",
   "metadata": {},
   "source": [
    "### BERT 양쪽을 문맥을 이해\n",
    "    - 빈칸채우기 \n",
    "\n",
    "####\n",
    "    - 데이터\n",
    "    - Hugging Face Transformers Pipeline(간단한 api)    \n",
    "    - 토크나이져 ( 텍스트 -> 토큰 ->숫자 ID)\n",
    "    - 사전학습된 모델(BERT , GPT-2, DistilBERT 등)\n",
    "    - 예측 결과(감성분석, 텍스트 생성, 문장 유사도)\n",
    "    - 후처리 평가(확률 변환, 정확도 계산)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed13b59",
   "metadata": {},
   "source": [
    "```\n",
    "\"I went to the [MASK] to buy some milk\"\n",
    "BERT 예측 : [MASK] = \"store\" ( 앞뒤 문맥 buy, milk를 보고 추론)\n",
    "```\n",
    "\n",
    "### 사전학습 방법\n",
    "    - MLM(Masked Language Model) : 문장의 15% 단어를 MASK 처리한다음 예측\n",
    "    - NSP(Next Stentece Prediction) : 두 문장이 연결되는지 판단\n",
    "```\n",
    "사전학습(대규모 텍스트)\n",
    "[CLS] token 추가\n",
    "특정 태스크 레이블로 학습(감성분석, QA 등)\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8658b8b3",
   "metadata": {},
   "source": [
    "### DistilBERT\n",
    "    - BERT 압축버전 : 속도는 2배 빠르다, 성능은 95% 유지함\n",
    "    - 두꺼운 교과서(BERT)의 핵심만 추린 요약본(DistilBERT)\n",
    "### 지식증류(Knowledge Distillation)\n",
    "    - Teacher 모델(BERT) : 소프트레이블 생성(확률분포)\n",
    "    - Student 모델(DistilBERT) : Teacher 출력을 모방\n",
    "### GPT-2\n",
    "    - 이전단어들을 보고 다음 단어를 예측    \n",
    "    - 단방향 Attention:\n",
    "    - Zero-shot Learning : 특정 태스트 학습 없이도 수행 가능\n",
    "    - GPT 시리즈\n",
    "        - GPT -1 : 117M 파라메터\n",
    "        - GPT -2 : 1.5B 파라메터\n",
    "        - GPT -3 : 175B 파라메터(Few-shot Learning)\n",
    "        - GPT -4 : 멀티모달\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545a9d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits : [2.5 1.  0.5]\n",
      "softmax : [0.73612472 0.16425163 0.09962365]\n",
      "softmax sum : 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def softmax(logits):\n",
    "    '''\n",
    "    로짓을 확률 분포로 변환\n",
    "    Args:\n",
    "        logits : 모델의 출력 점수( [2.5,1.0,0.5])\n",
    "    returns:\n",
    "        확률 분포(합이 1인 배열)\n",
    "    '''\n",
    "    # 수치 안정성을 위해 최대값을 빼줌(오버플로우 방지)\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / np.sum(exp_logits)\n",
    "logits = np.array([2.5,1.0,0.5])    \n",
    "probs = softmax(logits)\n",
    "print(f'logits : {logits}')\n",
    "print(f'softmax : {probs}')\n",
    "print(f'softmax sum : {np.sum(probs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b16c7",
   "metadata": {},
   "source": [
    "### WordPiece   Word + Piece\n",
    "    - 단어를 자주 등장하는 조각(piece) 단위로 잘라서 처리\n",
    "    - 기존토크나이져 대비 --> 더 잘게 쪼개자. (Subword)\n",
    "    - playing ----->  play + ##ing    ## 앞 조각에 붙는 서브워드 라는 의미\n",
    "    - 득템   ---------> 득 + 템"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485f8c4",
   "metadata": {},
   "source": [
    "WordPiece 토큰화 시뮬레이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b5e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb36ac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본문장 : I love natural language processing!\n",
      "토큰목록 : ['i', 'love', 'natural', 'language', 'processing', '!']\n",
      "토큰ID : [101, 1045, 2293, 3019, 2653, 6364, 999, 102]\n",
      "디코딩결과 : [CLS] i love natural language processing! [SEP]\n",
      "CSL토큰 : [CLS] -> 101\n",
      "SEP토큰 : [SEP] -> 102\n",
      "PAD토큰 : [PAD] -> 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\LLM311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# BERT 토크나이져 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# 테스트 문장\n",
    "sentence = 'I love natural language processing!'\n",
    "# 토큰화\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "token_ids = tokenizer.encode(sentence)\n",
    "# 디코딩\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(f'원본문장 : {sentence}')\n",
    "print(f'토큰목록 : {tokens}')\n",
    "print(f'토큰ID : {token_ids}')\n",
    "print(f'디코딩결과 : {decoded}')\n",
    "\n",
    "print(f'CSL토큰 : {tokenizer.cls_token} -> {tokenizer.cls_token_id}')\n",
    "print(f'SEP토큰 : {tokenizer.sep_token} -> {tokenizer.sep_token_id}')\n",
    "print(f'PAD토큰 : {tokenizer.pad_token} -> {tokenizer.pad_token_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d5632",
   "metadata": {},
   "source": [
    "BERT로 문장 유사도 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacec0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf63c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\LLM311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장1 : The cat is on the mat\n",
      "문장2 : A feline is sitting on a rug\n",
      "유사확률 : 0.90\n",
      "\n",
      "문장1 : I love pizza\n",
      "문장2 : Python is a programming language\n",
      "유사확률 : 0.04\n",
      "\n",
      "문장1 : He runs fast\n",
      "문장2 : She walks slowly\n",
      "유사확률 : 0.11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AutoTokenizer  지정한 모델 이름에 맞게 토크나이져를 자동으로 불러오는 클래스\n",
    "# AutoModelForSequenceClassification : 문장분류용 BERt모델을 자동으로 로드\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased-finetuned-mrpc')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased-finetuned-mrpc')\n",
    "def check_similarity(sentence1, sentence2):\n",
    "    '''\n",
    "    두 문장의 의미적 유사도를 판단\n",
    "    Returns:\n",
    "        유사확률(0~1 사이 값)\n",
    "    '''\n",
    "    #1 토큰화\n",
    "    inputs = tokenizer(sentence1,sentence2,return_tensors='pt')\n",
    "    #2 모델추론 \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    #3 softmax로 확률 변환\n",
    "        probs = torch.softmax(logits, dim=1)[0]  # 0은 배치 배치를 제거\n",
    "    #4 결과 반환\n",
    "    return{\n",
    "        'not_similar' : probs[0].item(),\n",
    "        'similar' : probs[1].item()\n",
    "    }\n",
    "# 테스트 케이스\n",
    "test_cases = [\n",
    "    (\"The cat is on the mat\", \"A feline is sitting on a rug\"),\n",
    "    (\"I love pizza\", \"Python is a programming language\"),\n",
    "    (\"He runs fast\", \"She walks slowly\")\n",
    "]\n",
    "for sent1,sent2 in test_cases:\n",
    "    result = check_similarity(sent1,sent2)\n",
    "    print(f'문장1 : {sent1}')\n",
    "    print(f'문장2 : {sent2}')\n",
    "    print(f\"유사확률 : {result['similar']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f3f78",
   "metadata": {},
   "source": [
    "GPT-2\n",
    "```\n",
    "다음 단어 후보와 확률\n",
    "The cat is on the 다음에 단어 후보 와 확률\n",
    "mat             0.4\n",
    "roof            0.25\n",
    "bed             0.15\n",
    "chair           0.10\n",
    "floor           0.05\n",
    "\n",
    "top-k  3  mat roof  bed\n",
    "top-p  0.8\n",
    "\n",
    "mat  0.40  -> 누적 0.40\n",
    "roof 0.25  -> 누적 0.65\n",
    "bed 0.15   -> 누적 0.80 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b3d6ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\LLM311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt : Once upon a time\n",
      "생성 : Once upon a time, there were seven or eight people who were doing it in their homes, and they said, \"Hey, that's how I\n",
      "\n",
      "prompt : In the year 2050,\n",
      "생성 : In the year 2050, the global temperature will rise to 1.3 C above pre-industrial levels and reach more than 2 C above the pre-\n",
      "\n",
      "prompt : The secret to happiness is\n",
      "생성 : The secret to happiness is to feel it. If it's not, nothing is.\n",
      "\n",
      "* * *\n",
      "\n",
      "A year ago, at least\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel,GPT2Tokenizer\n",
    "import torch\n",
    "# 모델 로드\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "def generate_text(prompt, max_length = 30):\n",
    "    '''\n",
    "    프롬프트 기반으로 텍스트 생성\n",
    "    Args:\n",
    "        prompt : 시작문장\n",
    "        max_length : 최대 토큰수\n",
    "    '''\n",
    "    #1 입력 토큰화\n",
    "    input_ids = tokenizer.encode(prompt,return_tensors='pt')\n",
    "    #2 생성(다양한 전략)\n",
    "    with torch.no_grad():  # 추론(평가)모드\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length = max_length,\n",
    "            num_return_sequences = 1, # 생성할 문장 수\n",
    "            temperature=0.8, # 창의성 조절\n",
    "            top_k=50, # 샘플링전략  상위 k개의 단어만 선택\n",
    "            top_p=0.95, # 누적확률 p 이상 단어만\n",
    "            do_sample = True,  #확률적 셈플링\n",
    "            pad_token_id = tokenizer.eos_token_id  # gpt2는 eos 토큰을 사용\n",
    "        )\n",
    "\n",
    "    #3 디코딩\n",
    "    generated_txt = tokenizer.decode(output[0],skip_special_tokens=True)\n",
    "    return generated_txt\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"In the year 2050,\",\n",
    "    \"The secret to happiness is\"\n",
    "]\n",
    "for prompt in prompts:\n",
    "    result = generate_text(prompt)\n",
    "    print(f'prompt : {prompt}')\n",
    "    print(f'생성 : {result}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa37a0d4",
   "metadata": {},
   "source": [
    "- BertTokenizer : 직접호출\n",
    "    - BERT 전용\n",
    "    - bert-base-uncased,  bert-base-cased 등 BERT계열만 지원\n",
    "    - 다른 모델에서는 사용 불가\n",
    "    - 모델전화시 코드 수정\n",
    "- AutoTokenizer : 자동선택\n",
    "    - BERT RoBERTa, GPT-2, T5 다양한 모델 지원\n",
    "    - from_pretraned에 모델이름을 넣으면 \n",
    "    - 어떤 클래스가 선택되었는지 모름->세부구현시 세부옵션 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a3da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 모델 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "\n",
    "#self-Attention 가중치 시각화\n",
    "sentence = 'The cat sat on the mat'\n",
    "# 1 토큰화\n",
    "inputs = tokenizer(sentence,return_tensors='pt')\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "# 2 모델 실행(Attention 추출)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    attentions = outputs.attentions  #12개 레이어의 Attention\n",
    "# 3 첫번째 레이어, 첫 번째 헤드 Attention 시각화\n",
    "attention = attentions[0][0,0].numpy()  \n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(\n",
    "    attention,\n",
    "    xticklabels=tokens,\n",
    "    yticklabels=tokens,\n",
    "    cmap = 'viridis',\n",
    "    annot = True,\n",
    "    fmt = '.2f',\n",
    "    cbar_kws={'label':'Attention Weight'}\n",
    ")\n",
    "plt.xlabel('key(참조 토큰)')\n",
    "plt.ylabel('Query(현재 토큰)')\n",
    "\n",
    "print(f'토큰목록 : {tokens}')\n",
    "print(f'Attention 행렬 크기 : {attentions.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "419da1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert 모델을 이용해서 영화리뷰 감성분석\n",
    "# 데이터셋 : NLTK movie_reviews\n",
    "# 모델 DistilBERT\n",
    "# 평가 : 분류리포트\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b2f0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews',quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57c21c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [movie_reviews.raw(id) for id in movie_reviews.fileids()]\n",
    "categoris = [ movie_reviews.categories(id)[0] for id in movie_reviews.fileids() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42c37d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos 1  eng 0\n",
    "y =  np.array([1 if ca == 'pos' else 0 for ca in categoris])\n",
    "x_train,x_test,y_train,y_test = train_test_split(reviews,y, stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52ae5ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\LLM311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\playdata2\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\playdata2\\miniconda3\\envs\\LLM311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af8dea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57b8d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측함수\n",
    "def predict_sentiment(texts, batch_size=0):\n",
    "    '''\n",
    "    배치단위로 감성 예측\n",
    "    Args:\n",
    "        texts: 리뷰 텍스트 리스트\n",
    "        batch_size:한번에 처리할 샘플 수\n",
    "    Returns:\n",
    "        예측 레이블 배열\n",
    "    '''\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    #배치단위 처리\n",
    "    num_batchs = (len(texts) + batch_size-1) // batch_size  # 103  20      (103+19)//20   (122)//20 \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(num_batchs),desc='예측 중'):\n",
    "            # 배치추출\n",
    "            batch_text = texts[ i*batch_size : (i+1)*batch_size  ]\n",
    "            # 토큰화(최대 512토큰, 패딩적용)\n",
    "            inputs = tokenizer(\n",
    "                batch_text,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length = 512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            inputs = { k: v.to(device) for k, v in inputs.items()  }\n",
    "            # 모델 예측\n",
    "            outputs =  model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            # 확률변환\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            preds =  probs.argmax(dim=-1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bb1dd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "예측 중:   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "예측 중: 100%|██████████| 32/32 [02:58<00:00,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          부정       0.76      0.92      0.83       250\n",
      "          긍정       0.90      0.70      0.79       250\n",
      "\n",
      "    accuracy                           0.81       500\n",
      "   macro avg       0.83      0.81      0.81       500\n",
      "weighted avg       0.83      0.81      0.81       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_sentiment(x_test,batch_size=16)\n",
    "print(classification_report(y_test, y_pred,target_names=['부정','긍정']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a45a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoTokenizer / AutoModel...  GPT-2  텍스트 생성   \n",
    "# 생성되 문장과 원본문장을  Bert를 이용해서 문장 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a3fc755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\LLM311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프롬프트 : ['Once upon a time', 'In the year 2050,', 'The secret to happiness is']\n",
      "생성 : Once upon a time, the world, without such evil, was a great place. And we can imagine, on the other hand, that it was\n",
      "유사도 : {'not_similar': 0.9278974533081055, 'similar': 0.07210253924131393}\n",
      "\n",
      "프롬프트 : ['Once upon a time', 'In the year 2050,', 'The secret to happiness is']\n",
      "생성 : In the year 2050, the rate of deforestation will reach 6 percent of the global average.\n",
      "\n",
      "The researchers found that one-fifth of people in\n",
      "유사도 : {'not_similar': 0.9527856707572937, 'similar': 0.04721428081393242}\n",
      "\n",
      "프롬프트 : ['Once upon a time', 'In the year 2050,', 'The secret to happiness is']\n",
      "생성 : The secret to happiness is to keep an eye on what is happening around you. A lot of people know it when they think of it and think '\n",
      "유사도 : {'not_similar': 0.9377418160438538, 'similar': 0.062258169054985046}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "# AutoModelForCausalLM 시퀀스의 다음단어를 앞 단어기반으로 한단계씩 예측  (자동 회귀 모델)  GPT GPT-Neo.. 등\n",
    "# AutoModelForSequenceClassification  문장 문서분류(Classification) -> 감성분석 스펨 문장유사도 등..  BERT RoBERTa DistilBERT\n",
    "\n",
    "#  텍스트 생성  GPT-2\n",
    "GPT_MODEL_NAME = 'gpt2'\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(GPT_MODEL_NAME)\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(GPT_MODEL_NAME)\n",
    "\n",
    "prompt = 'once upon a time'\n",
    "\n",
    "def generate_text(prompt,max_length=30):\n",
    "    input_ids = gpt_tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    with torch.no_grad():\n",
    "        output = gpt_model.generate(\n",
    "            input_ids,\n",
    "            max_length = max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.8,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=gpt_tokenizer.eos_token_id\n",
    "          )\n",
    "    return gpt_tokenizer.decode(output[0],skip_special_tokens=True)\n",
    "\n",
    "# BERT 모델(문장 유사도 MRPC)\n",
    "BERT_MODEL_NAME = 'bert-base-cased-finetuned-mrpc'\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(BERT_MODEL_NAME)\n",
    "def sentence_similarity(sent1,sent2):\n",
    "    inputs = bert_tokenizer(sent1,sent2,return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        logits = bert_model(**inputs).logits\n",
    "        probs = torch.softmax(logits,dim=-1)[0]\n",
    "    return{\n",
    "        'not_similar' : probs[0].item(),\n",
    "        'similar' : probs[1].item()\n",
    "    }\n",
    "\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"In the year 2050,\",\n",
    "    \"The secret to happiness is\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    # GPT-2 텍스트 생성\n",
    "    generated = generate_text(prompt)\n",
    "    #BERTㄹ 원본-생성문 유사도 계산\n",
    "    similarity = sentence_similarity(prompt,generated)\n",
    "\n",
    "    print(f'프롬프트 : {prompts}')\n",
    "    print(f'생성 : {generated}')\n",
    "    print(f'유사도 : {similarity}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8e8dc4",
   "metadata": {},
   "source": [
    "```\n",
    "- 현재 가상환경 목록 확인\n",
    "    - conda evn list\n",
    "- 가상환경 생성\n",
    "    - conda create -n 이름  python=3.11\n",
    "- 가상환경 활성화\n",
    "    - conda activate 이름\n",
    "- 필요한 라이브러리 로드\n",
    "    - pip install torch torchvision torchaudio\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678eccf5",
   "metadata": {},
   "source": [
    "파인튜닝 - 추가 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d30d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
