{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182d8fb0",
   "metadata": {},
   "source": [
    "### BERT 양쪽을 문맥을 이해\n",
    "    - 빈칸채우기 \n",
    "\n",
    "####\n",
    "    - 데이터\n",
    "    - Hugging Face Transformers Pipeline(간단한 api)    \n",
    "    - 토크나이져 ( 텍스트 -> 토큰 ->숫자 ID)\n",
    "    - 사전학습된 모델(BERT , GPT-2, DistilBERT 등)\n",
    "    - 예측 결과(감성분석, 텍스트 생성, 문장 유사도)\n",
    "    - 후처리 평가(확률 변환, 정확도 계산)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed13b59",
   "metadata": {},
   "source": [
    "```\n",
    "\"I went to the [MASK] to buy some milk\"\n",
    "BERT 예측 : [MASK] = \"store\" ( 앞뒤 문맥 buy, milk를 보고 추론)\n",
    "```\n",
    "\n",
    "### 사전학습 방법\n",
    "    - MLM(Masked Language Model) : 문장의 15% 단어를 MASK 처리한다음 예측\n",
    "    - NSP(Next Stentece Prediction) : 두 문장이 연결되는지 판단\n",
    "```\n",
    "사전학습(대규모 텍스트)\n",
    "[CLS] token 추가\n",
    "특정 태스크 레이블로 학습(감성분석, QA 등)\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8658b8b3",
   "metadata": {},
   "source": [
    "### DistilBERT\n",
    "    - BERT 압축버전 : 속도는 2배 빠르다, 성능은 95% 유지함\n",
    "    - 두꺼운 교과서(BERT)의 핵심만 추린 요약본(DistilBERT)\n",
    "### 지식증류(Knowledge Distillation)\n",
    "    - Teacher 모델(BERT) : 소프트레이블 생성(확률분포)\n",
    "    - Student 모델(DistilBERT) : Teacher 출력을 모방\n",
    "### GPT-2\n",
    "    - 이전단어들을 보고 다음 단어를 예측    \n",
    "    - 단방향 Attention:\n",
    "    - Zero-shot Learning : 특정 태스트 학습 없이도 수행 가능\n",
    "    - GPT 시리즈\n",
    "        - GPT -1 : 117M 파라메터\n",
    "        - GPT -2 : 1.5B 파라메터\n",
    "        - GPT -3 : 175B 파라메터(Few-shot Learning)\n",
    "        - GPT -4 : 멀티모달\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545a9d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits : [2.5 1.  0.5]\n",
      "softmax : [0.73612472 0.16425163 0.09962365]\n",
      "softmax sum : 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def softmax(logits):\n",
    "    '''\n",
    "    로짓을 확률 분포로 변환\n",
    "    Args:\n",
    "        logits : 모델의 출력 점수( [2.5,1.0,0.5])\n",
    "    returns:\n",
    "        확률 분포(합이 1인 배열)\n",
    "    '''\n",
    "    # 수치 안정성을 위해 최대값을 빼줌(오버플로우 방지)\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / np.sum(exp_logits)\n",
    "logits = np.array([2.5,1.0,0.5])    \n",
    "probs = softmax(logits)\n",
    "print(f'logits : {logits}')\n",
    "print(f'softmax : {probs}')\n",
    "print(f'softmax sum : {np.sum(probs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b16c7",
   "metadata": {},
   "source": [
    "### WordPiece   Word + Piece\n",
    "    - 단어를 자주 등장하는 조각(piece) 단위로 잘라서 처리\n",
    "    - 기존토크나이져 대비 --> 더 잘게 쪼개자. (Subword)\n",
    "    - playing ----->  play + ##ing    ## 앞 조각에 붙는 서브워드 라는 의미\n",
    "    - 득템   ---------> 득 + 템"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485f8c4",
   "metadata": {},
   "source": [
    "WordPiece 토큰화 시뮬레이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b5e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb36ac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본문장 : I love natural language processing!\n",
      "토큰목록 : ['i', 'love', 'natural', 'language', 'processing', '!']\n",
      "토큰ID : [101, 1045, 2293, 3019, 2653, 6364, 999, 102]\n",
      "디코딩결과 : [CLS] i love natural language processing! [SEP]\n",
      "CSL토큰 : [CLS] -> 101\n",
      "SEP토큰 : [SEP] -> 102\n",
      "PAD토큰 : [PAD] -> 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\LLM311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# BERT 토크나이져 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# 테스트 문장\n",
    "sentence = 'I love natural language processing!'\n",
    "# 토큰화\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "token_ids = tokenizer.encode(sentence)\n",
    "# 디코딩\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(f'원본문장 : {sentence}')\n",
    "print(f'토큰목록 : {tokens}')\n",
    "print(f'토큰ID : {token_ids}')\n",
    "print(f'디코딩결과 : {decoded}')\n",
    "\n",
    "print(f'CSL토큰 : {tokenizer.cls_token} -> {tokenizer.cls_token_id}')\n",
    "print(f'SEP토큰 : {tokenizer.sep_token} -> {tokenizer.sep_token_id}')\n",
    "print(f'PAD토큰 : {tokenizer.pad_token} -> {tokenizer.pad_token_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d5632",
   "metadata": {},
   "source": [
    "BERT로 문장 유사도 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacec0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf63c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\LLM311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장1 : The cat is on the mat\n",
      "문장2 : A feline is sitting on a rug\n",
      "유사확률 : 0.90\n",
      "\n",
      "문장1 : I love pizza\n",
      "문장2 : Python is a programming language\n",
      "유사확률 : 0.04\n",
      "\n",
      "문장1 : He runs fast\n",
      "문장2 : She walks slowly\n",
      "유사확률 : 0.11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AutoTokenizer  지정한 모델 이름에 맞게 토크나이져를 자동으로 불러오는 클래스\n",
    "# AutoModelForSequenceClassification : 문장분류용 BERt모델을 자동으로 로드\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased-finetuned-mrpc')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased-finetuned-mrpc')\n",
    "def check_similarity(sentence1, sentence2):\n",
    "    '''\n",
    "    두 문장의 의미적 유사도를 판단\n",
    "    Returns:\n",
    "        유사확률(0~1 사이 값)\n",
    "    '''\n",
    "    #1 토큰화\n",
    "    inputs = tokenizer(sentence1,sentence2,return_tensors='pt')\n",
    "    #2 모델추론 \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    #3 softmax로 확률 변환\n",
    "        probs = torch.softmax(logits, dim=1)[0]  # 0은 배치 배치를 제거\n",
    "    #4 결과 반환\n",
    "    return{\n",
    "        'not_similar' : probs[0].item(),\n",
    "        'similar' : probs[1].item()\n",
    "    }\n",
    "# 테스트 케이스\n",
    "test_cases = [\n",
    "    (\"The cat is on the mat\", \"A feline is sitting on a rug\"),\n",
    "    (\"I love pizza\", \"Python is a programming language\"),\n",
    "    (\"He runs fast\", \"She walks slowly\")\n",
    "]\n",
    "for sent1,sent2 in test_cases:\n",
    "    result = check_similarity(sent1,sent2)\n",
    "    print(f'문장1 : {sent1}')\n",
    "    print(f'문장2 : {sent2}')\n",
    "    print(f\"유사확률 : {result['similar']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f3f78",
   "metadata": {},
   "source": [
    "GPT-2\n",
    "```\n",
    "다음 단어 후보와 확률\n",
    "The cat is on the 다음에 단어 후보 와 확률\n",
    "mat             0.4\n",
    "roof            0.25\n",
    "bed             0.15\n",
    "chair           0.10\n",
    "floor           0.05\n",
    "\n",
    "top-k  3  mat roof  bed\n",
    "top-p  0.8\n",
    "\n",
    "mat  0.40  -> 누적 0.40\n",
    "roof 0.25  -> 누적 0.65\n",
    "bed 0.15   -> 누적 0.80 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b3d6ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\LLM311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt : Once upon a time\n",
      "생성 : Once upon a time, there were seven or eight people who were doing it in their homes, and they said, \"Hey, that's how I\n",
      "\n",
      "prompt : In the year 2050,\n",
      "생성 : In the year 2050, the global temperature will rise to 1.3 C above pre-industrial levels and reach more than 2 C above the pre-\n",
      "\n",
      "prompt : The secret to happiness is\n",
      "생성 : The secret to happiness is to feel it. If it's not, nothing is.\n",
      "\n",
      "* * *\n",
      "\n",
      "A year ago, at least\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel,GPT2Tokenizer\n",
    "import torch\n",
    "# 모델 로드\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "def generate_text(prompt, max_length = 30):\n",
    "    '''\n",
    "    프롬프트 기반으로 텍스트 생성\n",
    "    Args:\n",
    "        prompt : 시작문장\n",
    "        max_length : 최대 토큰수\n",
    "    '''\n",
    "    #1 입력 토큰화\n",
    "    input_ids = tokenizer.encode(prompt,return_tensors='pt')\n",
    "    #2 생성(다양한 전략)\n",
    "    with torch.no_grad():  # 추론(평가)모드\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length = max_length,\n",
    "            num_return_sequences = 1, # 생성할 문장 수\n",
    "            temperature=0.8, # 창의성 조절\n",
    "            top_k=50, # 샘플링전략  상위 k개의 단어만 선택\n",
    "            top_p=0.95, # 누적확률 p 이상 단어만\n",
    "            do_sample = True,  #확률적 셈플링\n",
    "            pad_token_id = tokenizer.eos_token_id  # gpt2는 eos 토큰을 사용\n",
    "        )\n",
    "\n",
    "    #3 디코딩\n",
    "    generated_txt = tokenizer.decode(output[0],skip_special_tokens=True)\n",
    "    return generated_txt\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"In the year 2050,\",\n",
    "    \"The secret to happiness is\"\n",
    "]\n",
    "for prompt in prompts:\n",
    "    result = generate_text(prompt)\n",
    "    print(f'prompt : {prompt}')\n",
    "    print(f'생성 : {result}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a3da2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
