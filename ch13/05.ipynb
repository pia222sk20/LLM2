{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83fe5008",
   "metadata": {},
   "source": [
    "```\n",
    "Attention Mechanism\n",
    "\n",
    "고양이가 잔다 - > the cat sleeps\n",
    "\n",
    "입력 : \"고양이가 잔다\"\n",
    "\n",
    "[전처리] -> <start> 고양이가 잔다 <end>\n",
    "\n",
    "[ENCODER] -> 각 단어마다 hidden state 생성\n",
    "\n",
    "[ATTENTION] -> 어디에 집중할지 계산\n",
    "\n",
    "[DECODER] -> 한 단어씩 생성\n",
    "\n",
    "출력 : the cat sleeps\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본\n",
    "입력 = \"고양이가 자요\"\n",
    "# 1단계 전처리\n",
    "전처리_후 = f'<start> {input} <end>'\n",
    "# 2단계 토큰화(단어->숫자)\n",
    "단어_사전 = {\n",
    "    '<pad>' : 0,\n",
    "    '<start>' : 1,\n",
    "    '<end>' : 2,\n",
    "    '고양이가' : 3,\n",
    "    '자요' : 4\n",
    "}\n",
    "# 최대길이 5대로 \n",
    "정수_시퀀스 = [1,3,4,2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa580e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 : EMBEDDING(숫자 -> 벡터)\n",
    "# 임베딩레이어 =========== 256차원\n",
    "정수_시퀀스 = [1,3,4,2,0]\n",
    "# Embedding.....\n",
    "임베딩_결과 = [\n",
    "    [0.2,-0.5, ..., 0.1]   #1 (<start>)의 벡터 (265)차원\n",
    "    ,[0.2,-0.5, ..., 0.3]  #3 (고양이가)의 벡터 (265)차원\n",
    "    ,[0.2,-0.5, ..., 0.4]  #4 (자요)의 벡터 (265)차원\n",
    "    ,[0.2,-0.5, ..., 0.2]  #2 (<end>)의 벡터 (265)차원\n",
    "    ,[0.0,0.0, ..., 0.0]  #0 (<pad>)의 벡터 (265)차원\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b949df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "LSTM = tf.keras.layer.LSTN()\n",
    "# STEP 2 : ENCODER(벡터-> Hidden States)\n",
    "# LSTM Encoder\n",
    "# 각 타임스텝마다 hidden state 생성\n",
    "\n",
    "# 초기상태\n",
    "h0 =[0,0,0, ..., 0] # 512차원 영벡터 \n",
    "c0 =[0,0,0, ..., 0] # 512차원 영벡터 \n",
    "\n",
    "# ============= 타임스텝 1 : <start> 처리 ================\n",
    "입력_t1 = [0.2,-0.5, ..., 0.1] # <start> 의 임베딩\n",
    "h1, c1 =  LSTM(입력_t1, h0, c0)\n",
    "# h1 = [0.1 .....] 512차원\n",
    "\n",
    "# ============= 타임스텝 2 : 고양이가 처리 ================\n",
    "입력_t2 = [0.2,-0.5, ..., 0.3] # 고양이가 의 임베딩\n",
    "h2, c2 =  LSTM(입력_t2, h1, c1)\n",
    "\n",
    "# ============= 타임스텝 3 : 자요 처리 ================\n",
    "입력_t3 = [0.2,-0.5, ..., 0.4] # 자요 의 임베딩\n",
    "h3, c3 =  LSTM(입력_t3, h2, c2)\n",
    "\n",
    "# ============= 타임스텝 4 : <end> 처리 ================\n",
    "입력_t4 = [0.2,-0.5, ..., 0.2] # <end> 의 임베딩\n",
    "h4, c4 =  LSTM(입력_t4, h3, c3)\n",
    "\n",
    "# ============= 타임스텝 4 : 패딩 (무시) ================\n",
    "입력_t5 = [0.0,0.0, ..., 0.0]\n",
    "h5, c5 =  LSTM(입력_t5, h4, c4)\n",
    "# h5는 사용 안함(마스킹)\n",
    "\n",
    "# ============ 인코더의 출력 =================\n",
    "encoder_output = [h1,h2,h3,h4,h5] # 모든 hidden states\n",
    "# 형태 [1,5,512]   [배치, 시퀀스,units]\n",
    "encoder_final_h = h4  # 마지막 hidden state(디코더 초기화용)\n",
    "encoder_final_c = c4  # 마지막 cell state\n",
    "\n",
    "# ================= 정리 ==================\n",
    "# <start>   ----> h1\n",
    "# 고양이가   --->   h2    \"고양이\"  정보를 압축\n",
    "# 자요       --->   h3    \"자요\"  정보를 압축\n",
    "# <end>      --->  h4     전체 문장 요약\n",
    "# <pad>      --->  h5   무시됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc1298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
