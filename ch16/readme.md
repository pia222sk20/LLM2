Ch 15. BERT 미세조정학습 (Fine-tuning)

## 목표
- BERT 사전학습 모델을 활용한 전이학습(Transfer Learning) 이해  
- HuggingFace Transformers 라이브러리를 이용한 토큰화 및 전처리 기법 습득  
- Trainer API와 PyTorch를 이용한 두 가지 미세조정 방법 학습  
- 감성 분석(Sentiment Analysis) 태스크에 BERT 적용 실습  

---

## 핵심 알고리즘

| 알고리즘/기법 | 한 줄 요약 |
|--------------|------------|
| **BERT Tokenizer (WordPiece)** | 단어를 subword 단위로 분해하여 OOV 문제 해결 |
| **Attention Mask** | 패딩된 토큰을 무시하도록 모델에 신호 전달 |
| **Token Type IDs** | 두 문장을 구분하기 위한 세그먼트 임베딩 |
| **[CLS] Token Pooling** | 문장 전체 표현을 추출하여 분류에 사용 |
| **Fine-tuning** | 사전학습된 BERT 가중치를 downstream task에 맞게 재학습 |
| **AdamW Optimizer** | Weight decay가 개선된 Adam 옵티마이저 |
| **Cross Entropy Loss** | 다중 클래스 분류 문제의 손실 함수 |
| **PyTorch Dataset/DataLoader** | 배치 학습을 위한 데이터 파이프라인 구축 |
| **Trainer API** | HuggingFace가 제공하는 고수준 학습 인터페이스 |

---

## 로드맵

```
┌─────────────────────────────────────────────────────────────┐
│ 1단계: BERT 토큰화 이해                                        │
│  └─ WordPiece 토큰화, Special Token ([CLS], [SEP], [PAD])   │
│  └─ input_ids, token_type_ids, attention_mask 생성           │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│ 2단계: 데이터 준비                                             │
│  └─ NLTK Movie Reviews 데이터셋 로드 (긍정/부정 리뷰)          │
│  └─ Train/Test Split (80:20)                                │
│  └─ PyTorch Dataset 클래스로 래핑                             │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│ 3단계: Method 1 - Trainer API 사용                            │
│  └─ BertForSequenceClassification 로드                       │
│  └─ TrainingArguments 설정 (epoch, batch size 등)            │
│  └─ Trainer 객체 생성 및 train() 호출                         │
│  └─ evaluate()로 성능 평가                                    │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│ 4단계: Method 2 - PyTorch 직접 구현                            │
│  └─ BertModel 로드 + 커스텀 분류 레이어 추가                    │
│  └─ DataLoader로 배치 처리                                    │
│  └─ 학습 루프 직접 구현 (forward, loss, backward, step)       │
│  └─ 테스트 데이터로 평가                                       │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│ 5단계: 결과 비교 및 응용                                       │
│  └─ 두 방법의 장단점 비교                                      │
│  └─ 다른 downstream task 적용 가능성 탐색                      │
└─────────────────────────────────────────────────────────────┘
```

---

## 학습 결과 요약

| 방법 | 테스트 정확도 | 특징 |
|------|----------------|--------|
| **Trainer API** | **87.0%** | 간단하고 빠른 구현, 자동화된 학습 관리 |
| **PyTorch 직접 구현** | **88.25%** | 세밀한 제어 가능, 커스텀 아키텍처 구현 용이 |

---

## 핵심 개념 정리

### 전이학습 (Transfer Learning)
대규모 말뭉치로 사전학습된 **BERT**를 가져와  
특정 downstream task(감성분석 등)에 맞게 **미세조정(Fine-tuning)** 하는 기법.

---

### 두 가지 구현 방식

#### **1) High-level (Trainer API 기반)**
- 빠른 프로토타입 개발에 매우 적합  
- 모델 저장/평가/로그 관리 자동화  
- 표준 텍스트 분류 태스크에 쉽게 활용 가능  

#### **2) Low-level (PyTorch 직접 구현)**
- 학습 루프를 직접 구현하여 완전한 제어 제공  
- 연구 목적 및 커스터마이징에 적합  
- 모델 구조/optimizer/스케줄러 등 세세한 조정 가능  

---

### [CLS] 토큰
문장의 첫 번째 위치에 추가되는 특수 토큰으로  
BERT는 이 토큰의 hidden state가 문장 전체 의미를 잘 표현한다고 가정.  
→ 문장 분류 태스크에서 대표적으로 사용됨.