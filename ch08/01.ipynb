{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d92387",
   "metadata": {},
   "source": [
    "### ÏûêÏó∞Ïñ¥ Í∞êÏÑ±Î∂ÑÏÑù\n",
    "- Í∞êÏÑ±ÏÇ¨Ï†Ñ Í∏∞Î∞ò : ÎØ∏Î¶¨Ï†ïÏùòÎêú Í∞êÏÑ± Îã®Ïñ¥ ÏÇ¨Ï†Ñ ÏÇ¨Ïö©(Í∑úÏπô Í∏∞Î∞ò)\n",
    "    - TextBlob, AFINNm VADER\n",
    "- Î®∏Ïã†Îü¨Îãù Í∏∞Î∞ò : Îç∞Ïù¥ÌÑ∞Î°úÎ∂ÄÌÑ∞ Ìå®ÌÑ¥ ÌïôÏäµ(ÌÜµÍ≥Ñ Í∏∞Î∞ò)\n",
    "    - TF-IDF Î≤°ÌÑ∞Ìôî\n",
    "    - ÏÑ†ÌòïÌöåÍ∑Ä\n",
    "    - Î°úÏßÄÏä§Ìã±ÌöåÍ∑Ä\n",
    "    - F1 Score, Recison, Recall  -> classification report\n",
    "### ÏÇ¨Ïö© Îç∞Ïù¥ÌÑ∞\n",
    "- NLTK ÏòÅÌôî Î¶¨Î∑∞(2000Í∞ú)\n",
    "- Îã§ÏùåÏòÅÌôîÎ¶¨Î∑∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c10d0c",
   "metadata": {},
   "source": [
    "#### ÏïåÍ≥†Î¶¨Ï¶ò\n",
    " - TextBlob     ÏÇ¨Ï†ÑÍ∏∞Î∞ò Í∞êÏÑ±Î∂ÑÏÑù\n",
    " - AFINN        Í∞êÏ†ï Ï†êÏàò Îß§Ìïë\n",
    " - VADER(Valence Aware Dictionary)  ÏÜåÏÖúÎØ∏ÎîîÏñ¥ ÏµúÏ†ÅÌôî Í∞êÏÑ±Î∂ÑÏÑù\n",
    " - TF-IDF       ÌÖçÏä§Ìä∏ Î≤°ÌÑ∞Ìôî\n",
    " - Multinomial Naive Bayes          ÌôïÎ•† Í∏∞Î∞ò Î∂ÑÎ•ò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c1beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob\n",
    "# Ïù¥ ÏòÅÌôîÎäî Ï†ïÎßê Ï¢ãÍ≥† Ïû¨ÎØ∏ÏûàÎã§\n",
    "    # Ï¢ãÎã§ +1(Í∏çÏ†ï)\n",
    "    # Ïû¨ÎØ∏ÏûàÎã§ +1(Í∏çÏ†ï)\n",
    "    # +2 > 0 --> Í∏çÏ†ï(pos) Î∂ÑÎ•ò\n",
    "# Polarity(Í∑πÏÑ±ÎèÑ)  (Í∏çÏ†ïÎã®Ïñ¥Ïàò Í∞úÏàò - Î∂ÄÏ†ïÎã®Ïñ¥ Í∞úÏàò) / Ï†ÑÏ≤¥ Îã®Ïñ¥ Í∞úÏàò\n",
    "# -1.0 ~ +1.0\n",
    "# 0 Ï§ëÎ¶Ω\n",
    "# Subjectivity(Ï£ºÍ¥ÄÏÑ±) ÌèâÍ∞ÄÎåÄÏÉÅ Îã®Ïñ¥ ÎπÑÏú®\n",
    "# 0.0 ~ 1.0\n",
    "# 0 : Í∞ùÍ¥ÄÏ†Å   1 : Ï£ºÍ¥ÄÏ†Å\n",
    "\n",
    "# Î¨∏Îß•Î¨¥ÏãúÌïòÍ≥† Îã®Ïñ¥ Í∑πÏÑ±Îßå Í≥†Î†§\n",
    "# Ïù¥ ÏòÅÌôîÎäî ÎÇòÏÅòÏßÄ ÏïäÎã§  -> ÎÇòÏÅòÎã§(-)  ÏïäÎã§(-) Î°ú Ïù∏Ïãù\n",
    "# Îã®Ï†ê : Îπ†Î•∏ ÏÜçÎèÑ, ÌïôÏäµ Î∂àÌïÑÏöî\n",
    "# ÏÇ¨Ïö© : Ïã§ÏãúÍ∞Ñ Í∞êÏÑ±Î∂ÑÏÑù,  Ïä§Ìä∏Î¶¨Î∞çÎç∞Ïù¥ÌÑ∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcbc601",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515bcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\"TextBlob is amazingly simple to use.\"), Sentence(\"What a wonderful library for NLP!\")]\n",
      "['TextBlob', 'is', 'amazingly', 'simple', 'to', 'use', 'What', 'a', 'wonderful', 'library', 'for', 'NLP']\n",
      "[('TextBlob', 'NNP'), ('is', 'VBZ'), ('amazingly', 'RB'), ('simple', 'JJ'), ('to', 'TO'), ('use', 'VB'), ('What', 'WP'), ('a', 'DT'), ('wonderful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('NLP', 'NNP')]\n",
      "['textblob', 'wonderful library', 'nlp']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "polarity : 0.5\n",
      "subjectivity : 0.6785714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\playdata2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "text = \"TextBlob is amazingly simple to use. What a wonderful library for NLP!\"\n",
    "blob =  TextBlob(text)\n",
    "print(blob.sentences)\n",
    "print(blob.words)\n",
    "print(blob.tags)\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "print(blob.noun_phrases)\n",
    "print('-'*100)\n",
    "# Í∞êÏÑ±Î∂ÑÏÑù\n",
    "blob.sentiment\n",
    "print(f'polarity : {blob.sentiment.polarity}')\n",
    "print(f'subjectivity : {blob.sentiment.subjectivity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFINN(Lexicon-Based)  Í∞êÏÑ±ÏÇ¨Ï†Ñ\n",
    "# Í∞Å Îã®Ïñ¥Ïùò -5 ~ +5Ïùò Ï†êÏàòÎ•º Î∂ÄÏó¨ÌïòÍ≥† Ìï©ÏÇ∞\n",
    "# Ïù¥ ÏòÅÌôîÎäî Ï¢ãÏßÄÎßå Ï¢ãÏßÄ ÏïäÏùÄ Î∂ÄÎ∂ÑÎèÑ ÏûàÎã§\n",
    "    # Ï¢ãÎã§ +3  Ï¢ãÎã§ +3  ÎÇòÏÅòÎã§ -3  =  +3 > 0 Í∏çÏ†ï\n",
    "# score = sum(word_sentiment_value)\n",
    "# Î∂ÑÎ•òÍ∑úÏπô score > 0 Í∏çÏ†ï  score < 0 Î∂ÄÏ†ï  \n",
    "# Ïù¥Î™®Ìã∞ÏΩòÏßÄÏõê\n",
    "# Í∞ïÎèÑÌëúÌòÑ Ïù∏Ïãù very , really Îì±\n",
    "\n",
    "# Í∞ïÏ°∞ ÏàòÏ†ïÏûê(intensifiers)\n",
    "    # Îß§Ïö∞Ï¢ãÎã§ = 1.5X(Ï¢ãÎã§Ïùò Ï†êÏàò)\n",
    "\n",
    "# AFINN vs TextBlob\n",
    "# AFINN : Îçî Ï†ïÌôïÌïú Ï†êÏàò Îß§Ìïë\n",
    "# TextBlob : Îçî ÏùºÎ∞òÏ†ÅÏù∏ Ï†ëÍ∑º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19e5c1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 4.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from afinn import Afinn\n",
    "af = Afinn()\n",
    "text1 = \"TextBlob is amazingly simple to use\"\n",
    "text2 = \"What a wonderful library for NLP!\"\n",
    "score1 = af.score(text1)\n",
    "score2 = af.score(text2)\n",
    "score1, score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afb4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER  ÏáºÏÑ§ ÎØ∏ÎîîÏñ¥ ÌÖçÏä§Ìä∏Ïóê ÏµúÏ†ÅÌôî\n",
    "# Ïù¥ ÏòÅÌôîÎäî Ï†ïÎßêÏ†ïÎßê ÌõåÎ•≠Ìï¥!!!\n",
    "# ÌõåÎ•≠ÌïòÎã§ (Í∏∞Î≥∏) + 0.7   Ï†ïÎßêÏ†ïÎßê (Í∞ïÏ°∞)x1.5\n",
    "# !!! (Î¨∏Ïû•Î∂ÄÌò∏Í∞ïÏ°∞) x1.2\n",
    "# 4Í∞úÏùò Í∞êÏ†ï ÏßÄÏàò\n",
    "    # positive Í∏çÏ†ï ÌôïÎ•† 0 ~ 1\n",
    "    # nagative Î∂ÄÏ†ï ÌôïÎ•†\n",
    "    # neutral Ï§ëÎ¶Ω ÌôïÎ•†\n",
    "    # compound Ï¢ÖÌï©Ï†êÏàò -1 ~ 1\n",
    "# score = compound_score / sqrt(compound_score**2 + 0.0625)\n",
    "# score >=0.05 Í∏çÏ†ï\n",
    "# score <=-0.05 Î∂ÄÏ†ï  \n",
    "# Í∑∏ ÏÇ¨Ïù¥Îäî Ï§ëÎ¶Ω\n",
    "# ÎåÄÏÜåÎ¨∏Ïûê Íµ¨Î∂Ñ  AMAZING amazing Îã§Î•∏ Ï†êÏàò\n",
    "# :) Í∏çÏ†ï    :-(  Î∂ÄÏ†ï\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b65a54db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\playdata2\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adceed6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î¨∏Ïû• : I love this product! It's absolutely amazing üòç\n",
      "Ï†êÏàò : {'neg': 0.0, 'neu': 0.318, 'pos': 0.682, 'compound': 0.862}\n",
      "Î¨∏Ïû• : This is the worst movie I've ever seen...\n",
      "Ï†êÏàò : {'neg': 0.369, 'neu': 0.631, 'pos': 0.0, 'compound': -0.6249}\n",
      "Î¨∏Ïû• : The food was okay, not great but not bad either.\n",
      "Ï†êÏàò : {'neg': 0.149, 'neu': 0.487, 'pos': 0.364, 'compound': 0.4728}\n",
      "Î¨∏Ïû• : I‚Äôm REALLY happy with the results!!!\n",
      "Ï†êÏàò : {'neg': 0.0, 'neu': 0.472, 'pos': 0.528, 'compound': 0.7651}\n",
      "Î¨∏Ïû• : Not good at all. I‚Äôm disappointed.\n",
      "Ï†êÏàò : {'neg': 0.579, 'neu': 0.421, 'pos': 0.0, 'compound': -0.6711}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyer = SentimentIntensityAnalyzer()\n",
    "sentences = [\n",
    "    \"I love this product! It's absolutely amazing üòç\",\n",
    "    \"This is the worst movie I've ever seen...\",\n",
    "    \"The food was okay, not great but not bad either.\",\n",
    "    \"I‚Äôm REALLY happy with the results!!!\",\n",
    "    \"Not good at all. I‚Äôm disappointed.\",\n",
    "]\n",
    "for s in sentences:\n",
    "    scores = analyer.polarity_scores(s)\n",
    "    print(f'Î¨∏Ïû• : {s}')\n",
    "    print(f'Ï†êÏàò : {scores}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a9bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from textblob import TextBlob\n",
    "from afinn import Afinn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# nltk Îç∞Ïù¥ÌÑ∞ Îã§Ïö¥Î°úÎìú\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# ÏòÅÌôî Î¶¨Î∑∞ Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "fileids = movie_reviews.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a601141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50, 50)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = [movie_reviews.raw(fileid) for fileid in  fileids[:50]] + \\\n",
    "        [movie_reviews.raw(fileid) for fileid in  fileids[-50:]]\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in  fileids[:50]] + \\\n",
    "        [movie_reviews.categories(fileid)[0] for fileid in  fileids[-50:]]\n",
    "len(reviews), categories.count('pos'), categories.count('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2730af27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï†ïÌôïÎèÑ : 0.6\n"
     ]
    }
   ],
   "source": [
    "# 1 TextBlob\n",
    "def sentiment_textblob(docs):\n",
    "    return ['pos' if TextBlob(doc).sentiment.polarity>0 else 'neg' for doc in docs ]\n",
    "predictions_textblob = sentiment_textblob(reviews)\n",
    "accuracy_textblob = accuracy_score(categories,predictions_textblob)\n",
    "print(f'Ï†ïÌôïÎèÑ : {accuracy_textblob:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89461d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï†ïÌôïÎèÑ : 0.7\n"
     ]
    }
   ],
   "source": [
    "# 2  AFINN\n",
    "def sentiment_afinn(docs):\n",
    "    afn = Afinn(emoticons=True)\n",
    "    return [ 'pos' if afn.score(doc) > 0 else 'neg'  for doc in docs]\n",
    "predictions = sentiment_afinn(reviews)\n",
    "accuracy = accuracy_score(categories,predictions)\n",
    "print(f'Ï†ïÌôïÎèÑ : {accuracy:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14e895b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï†ïÌôïÎèÑ : 0.6\n"
     ]
    }
   ],
   "source": [
    "# 3 VADER\n",
    "def sentiment_vader(docs):\n",
    "    analyer = SentimentIntensityAnalyzer()\n",
    "    return [ 'pos' if analyer.polarity_scores(doc)['compound'] > 0 else 'neg'  for doc in docs]\n",
    "predictions = sentiment_vader(reviews)\n",
    "accuracy = accuracy_score(categories,predictions)\n",
    "print(f'Ï†ïÌôïÎèÑ : {accuracy:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5c7876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î®∏Ïã†Îü¨Îãù Í∏∞Î∞ò Í∞êÏÑ±Î∂ÑÏÑù \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÎÇòÏù¥Î∏å Î≤†Ïù¥Ï¶à\n",
    "\n",
    "# Î≤†Ïù¥Ï¶à Ï†ïÎ¶¨\n",
    "# \"Ï¢ãÎã§\" Îã®Ïñ¥Î•º Î≥∏ÌõÑ Ïù¥ Î¶¨Î∑∞Í∞Ä Í∏çÏ†ïÏùº ÌôïÎ•†\n",
    "# p(Í∏çÏ†ï | \"Ï¢ãÎã§\") = p(\"Ï¢ãÎã§\" | Í∏çÏ†ï) x p(Í∏çÏ†ï) / p('Ï¢ãÎã§')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82da8ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
    "dataset = train_test_split(reviews, categories, test_size=0.2,random_state=42, stratify=categories)\n",
    "len(dataset[0]),len(dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3268bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf Î≤°ÌÑ∞Ìôî\n",
    "vectorizer =  TfidfVectorizer(max_features=1000)\n",
    "x_train = vectorizer.fit_transform(dataset[0])\n",
    "x_test = vectorizer.transform(dataset[1])\n",
    "\n",
    "y_train = dataset[2]\n",
    "y_test = dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6366c065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.58      0.70      0.64        10\n",
      "         pos       0.62      0.50      0.56        10\n",
      "\n",
      "    accuracy                           0.60        20\n",
      "   macro avg       0.60      0.60      0.60        20\n",
      "weighted avg       0.60      0.60      0.60        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. mnb\n",
    "mnb_clf = MultinomialNB()\n",
    "mnb_clf.fit(x_train,y_train)\n",
    "predict = mnb_clf.predict(x_test)\n",
    "print( classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3dce656b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.57      0.80      0.67        10\n",
      "         pos       0.67      0.40      0.50        10\n",
      "\n",
      "    accuracy                           0.60        20\n",
      "   macro avg       0.62      0.60      0.58        20\n",
      "weighted avg       0.62      0.60      0.58        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logisticregression\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(x_train,y_train)\n",
    "predict = lr_clf.predict(x_test)\n",
    "print( classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e6831a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏÑ±Îä•Ìñ•ÏÉÅ\n",
    "# ÏÜåÎ¨∏ÏûêÎ≥ÄÌôò - Ïó∞ÏÜçÎêú Î¨∏ÏûêÏó¥Ï§ëÏóê 3Í∏ÄÏûê Ïù¥ÏÉÅ - Ïñ¥Í∞ÑÏ∂îÏ∂ú(ÌòïÌÉúÏÜåÎ∂ÑÏÑù) - Î∂àÏö©Ïñ¥ Ï†úÍ±∞\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4709df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\LLM\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def custom_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    tokenizer =  RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    porter = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [porter.stem(token) for token in tokens if token not in stop_words]\n",
    "vector = TfidfVectorizer(\n",
    "    tokenizer  = custom_tokenizer\n",
    "    ,max_features=1000\n",
    "    ,min_df=5\n",
    "    ,max_df=0.5\n",
    "    ,token_pattern = r\"[\\w']{3,}\"\n",
    "    ,ngram_range = (1,1)\n",
    ")\n",
    "x_train = vector.fit_transform(dataset[0])\n",
    "x_test = vector.transform(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e347073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):    \n",
    "    model.fit(x_train,y_train)\n",
    "    predict = model.predict(x_test)\n",
    "    print( classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7fe1e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.80      0.80      0.80        10\n",
      "         pos       0.80      0.80      0.80        10\n",
      "\n",
      "    accuracy                           0.80        20\n",
      "   macro avg       0.80      0.80      0.80        20\n",
      "weighted avg       0.80      0.80      0.80        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "435739d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.71      0.50      0.59        10\n",
      "         pos       0.62      0.80      0.70        10\n",
      "\n",
      "    accuracy                           0.65        20\n",
      "   macro avg       0.66      0.65      0.64        20\n",
      "weighted avg       0.66      0.65      0.64        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21438afa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
