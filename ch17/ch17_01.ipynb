{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9gQ3gP62ZcW"
      },
      "source": [
        "KoBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2dIQFDy5LRS"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4z73QTC3s3-"
      },
      "outputs": [],
      "source": [
        "%pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrRItwWK4vyb"
      },
      "outputs": [],
      "source": [
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "tokenizer.encode(\"한국어 모델을 공유합니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHcNdKPf6YoM"
      },
      "outputs": [],
      "source": [
        "tokenizer('한국어 모델을 공유합니다.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVRLYpSS2ba_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertModel\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "text = \"한국어 모델을 공유합니다.\"\n",
        "inputs = tokenizer.batch_encode_plus([text])\n",
        "out = model(input_ids = torch.tensor(inputs['input_ids']),\n",
        "              attention_mask = torch.tensor(inputs['attention_mask']))\n",
        "out.pooler_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuuCKmL66_6B"
      },
      "outputs": [],
      "source": [
        "# 데이터셋클래스\n",
        "import torch\n",
        "class OurDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings, labels):\n",
        "    self.encodings = encodings\n",
        "    self.labels = labels\n",
        "  def __getitem__(self, idx):\n",
        "    item = {key: torch.tensor(val[idx].clone().detach()) for key, val in self.encodings.items()}\n",
        "    item['labels'] = torch.tensor(self.labels[idx])\n",
        "    return item\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGt5Er_I-VHZ"
      },
      "outputs": [],
      "source": [
        "# 데이터 로드 및 분할(여기서는 성능상 일부 데이터만 사용)\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "# 데이터셋 로드\n",
        "url = \"https://drive.google.com/uc?id=1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o\"\n",
        "df = pd.read_csv(url)\n",
        "print(len(df))\n",
        "df = df.sample(frac=0.1)\n",
        "print(len(df))\n",
        "X = df.review.tolist()\n",
        "y = (df.rating >= 6).values.astype(int)\n",
        "x_,x_test,y_,y_test = train_test_split(X,y,stratify=y,random_state=42,test_size=0.2)\n",
        "x_train,x_val,y_train,y_val = train_test_split(x_,y_,stratify=y_, random_state=42,test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxrAllZS8rM_",
        "outputId": "bf96bff0-febe-4f88-d5ab-f14ab56fd92b"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# 토큰화\n",
        "train_input = tokenizer(x_train, truncation=True, padding=True,return_tensors='pt')\n",
        "val_input = tokenizer(x_val, truncation=True, padding=True, max_length=512,return_tensors='pt')\n",
        "test_input = tokenizer(x_test, truncation=True, padding=True, max_length=512,return_tensors='pt')\n",
        "# DataSet 생성\n",
        "train_dataset = OurDataset(train_input,y_train)\n",
        "val_dataset = OurDataset(val_input,y_train)\n",
        "test_dataset = OurDataset(test_input,y_train)\n",
        "# 데이터로더 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=8)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "# KoBERT 한국어 전용 모델 로드\n",
        "from transformers import BertModel\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "# BERT를 포함한 신경망 모델\n",
        "import torch.nn as nn\n",
        "class MyModel(nn.Module):\n",
        "  def __init__(self,prefrained_model, token_size, num_labels):\n",
        "    super(MyModel,self).__init__()\n",
        "    self.prefrained_model=prefrained_model\n",
        "    self.token_size=token_size\n",
        "    self.num_labels=num_labels\n",
        "    # 분류기 정의\n",
        "    self.clf = nn.Linear(self.token_size,self.num_labels)\n",
        "  def forward(self,inputs):\n",
        "    outputs = self.prefrained_model(**inputs)  # [batch,embeding_dim,num_labels]\n",
        "    bert_clf_token = outputs.last_hidden_state[:,0,:]\n",
        "\n",
        "    return self.clf(bert_clf_token)\n",
        "model = MyModel(model,token_size = model.config.hidden_size,num_labels=2)\n",
        "# 학습 - 미니배치\n",
        "  # device\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available()  else 'cpu'\n",
        "model.to(device)\n",
        "  # optimize\n",
        "# AdamW import\n",
        "from torch.optim import AdamW\n",
        "# from torch. import AdamW\n",
        "optim = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "  # 학습 스케줄러\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer=optim,\n",
        "                                            num_training_steps = len(train_loader),\n",
        "                                            num_warmup_steps=200)\n",
        "  # epoch수만큼 loop\n",
        "import numpy as np\n",
        "total_loss = 0\n",
        "for batch in train_loader:\n",
        "  model.train()\n",
        "  optim.zero_grad()\n",
        "  # 배치에서는 label을 제외하고 입력만 추출\n",
        "  inputs = {k : v.to(device) for k,v in batch.items() if k != 'labels'}\n",
        "  labels = batch['labels'].to(device)\n",
        "  outputs = model(inputs)\n",
        "  outputs = np.argmax(outputs,dim=-1)\n",
        "  print(outputs.shape, labels.shape)\n",
        "\n",
        "#   loss = criterion(outputs, labels)\n",
        "#   loss.backward()\n",
        "#   optim.step()\n",
        "#   scheduler.step()\n",
        "#   total_loss += loss.item()\n",
        "# print(total_loss/len(train_loader)  )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
