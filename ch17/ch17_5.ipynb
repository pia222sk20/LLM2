{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mf66DrCJG3P"
      },
      "source": [
        "개체명 인식 : NER\n",
        "  - 텍스트에서 특정 의미를 가진 단어나 구절을 찾아내고 분류하는 작업"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZEcozbaJRht"
      },
      "outputs": [],
      "source": [
        "# 홍길동은 2025년 11월 19일 서울시청에서 삼성전자 직원을 만났다\n",
        "# 홍길동 - [인명]\n",
        "# 2024년 1월 15일 - [날자]\n",
        "# 서울시청 - [지명]\n",
        "# 삼성전자 -[기관명]\n",
        "\n",
        "# 활용분야\n",
        "  # 뉴스기사 : 기사에서 인물, 장소, 기관 자동추출\n",
        "  # 의료문서 : 병명, 약물명, 증상\n",
        "  # 계약서 : 회사명, 날자 , 금액\n",
        "  # 쳇봇 : 사용자 질문에 핵심정보 파악\n",
        "\n",
        "# BIO 태깅\n",
        "# B(Begin)   개체 시작\n",
        "# I(Inside)  개체 내부\n",
        "# O(Outside) 개체가 아님"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6y2SJF4JPJb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaB44bqEMCIz",
        "outputId": "8eb0251e-8630-4c9c-a288-6c1ac099209c"
      },
      "outputs": [],
      "source": [
        "# Bio 태깅\n",
        "tokens = [\"김철수는\", \"2024년\", \"1월\", \"15일\", \"서울시청에서\", \"삼성전자\", \"직원을\", \"만났다\"]\n",
        "bio_tags = [\"B-PER\", \"B-DAT\", \"I-DAT\", \"I-DAT\", \"B-LOC\", \"B-ORG\", \"O\", \"O\"]\n",
        "for token, tag in zip(tokens, bio_tags):\n",
        "  if tag.startswith('B-'):\n",
        "    desc = f\"'{tag[2:]}' 개체의 시작\"\n",
        "  elif tag.startswith('I-'):\n",
        "    desc = f\"'{tag[2:]}' 개체의 내부\"\n",
        "  else:\n",
        "    desc = \"개체가 아님\"\n",
        "  print(f\"{token:12} | {tag:8} | {desc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbYNDhhVMNzG"
      },
      "outputs": [],
      "source": [
        "# 학습데이터\n",
        "train_sentences = [\n",
        "    [\"김철수는\", \"서울에\", \"산다\"],\n",
        "    [\"이영희는\", \"2024년에\", \"부산으로\", \"이사했다\"],\n",
        "    [\"삼성전자는\", \"대한민국의\", \"대기업이다\"],\n",
        "    [\"박지성은\", \"축구선수다\"],\n",
        "    [\"2025년\", \"1월\", \"1일은\", \"새해다\"],\n",
        "]\n",
        "\n",
        "train_labels = [\n",
        "    [\"B-PER\", \"B-LOC\", \"O\"],\n",
        "    [\"B-PER\", \"B-DAT\", \"B-LOC\", \"O\"],\n",
        "    [\"B-ORG\", \"B-LOC\", \"O\"],\n",
        "    [\"B-PER\", \"O\"],\n",
        "    [\"B-DAT\", \"I-DAT\", \"I-DAT\", \"O\"],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGxs0CeWNx6Z",
        "outputId": "8ab460c0-8613-49d4-99a0-9d15344c1ed6"
      },
      "outputs": [],
      "source": [
        "from ast import mod\n",
        "# 토크나이져\n",
        "MODEL_NAME = 'skt/kobert-base-v1'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "text = '김철수는 서울에 산다'\n",
        "#토크나이져\n",
        "tokens = tokenizer.tokenize(text)\n",
        "# 인코딩\n",
        "encoded = tokenizer(text,return_tensors='pt')\n",
        "encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD7FqBCDO8Av"
      },
      "outputs": [],
      "source": [
        "# NER 모델 4단계로 구성\n",
        "# 1 입력 텍스트\n",
        "# 2. koBERT 인코더   문장의 의미를 이해\n",
        "# 3. 분류기(Linear)  예측\n",
        "# 4. 출력 라벨  B-PER O B-LOC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuU-OUqdPU7p",
        "outputId": "726c65dc-e899-47c8-c104-16185f105d87"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "class SimpleNERModel(nn.Module):\n",
        "  def __init__(self, num_labels) -> None:\n",
        "    super(SimpleNERModel, self).__init__()\n",
        "    self.num_labels = num_labels\n",
        "    self.bert = AutoModel.from_pretrained(MODEL_NAME)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.clf = nn.Linear(self.bert.config.hidden_size,  self.num_labels)\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    # kobert로 문자 인코딩\n",
        "    outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "    # 마지막 은닉상태 추출\n",
        "    sequence_output =  outputs.last_hidden_state\n",
        "    # Dropout 적용\n",
        "    sequence_output = self.dropout(sequence_output)\n",
        "    # 분류기\n",
        "    logits = self.clf(sequence_output)\n",
        "    return logits\n",
        "# 라벨의 개수\n",
        "label_list = sorted(list(set([data for i in train_labels for data in i])))\n",
        "label2id = { label:i for i, label in enumerate(label_list)}\n",
        "id2label = { i:label for i, label in enumerate(label_list)}\n",
        "model = SimpleNERModel(num_labels=len(label_list))\n",
        "\n",
        "# 모델 학습\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "# 순전파 테스트\n",
        "sample_sentence = train_sentences[0]\n",
        "sample_label = train_labels[0]\n",
        "print(f'테스트 문장 {' '.join(sample_sentence)}')\n",
        "print(f'정답 라벨 {' '.join(sample_label)}')\n",
        "\n",
        "encoding = tokenizer(sample_sentence, return_tensors='pt',truncation=True,\n",
        "          padding=True,max_length=32,is_split_into_words=True)\n",
        "input_ids = encoding['input_ids'].to(device)\n",
        "attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  logits = model(input_ids, attention_mask)\n",
        "  predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "word_ids =  encoding.word_ids(batch_index=0)\n",
        "pred_labels = []\n",
        "\n",
        "for i, word_idx in enumerate(word_ids):\n",
        "  if word_idx is not None and i < len(predictions[0]):\n",
        "    pred_label = id2label[ predictions[0][i].item() ]\n",
        "    if word_idx < len(sample_sentence):\n",
        "      print(f' {sample_sentence[word_idx]:10} -> {pred_label:8} 정답 : {sample_label[word_idx]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8kjEVFlBUeY"
      },
      "outputs": [],
      "source": [
        "# 학습 DataSet\n",
        "from torch.utils.data import Dataset\n",
        "class NerDataSet(Dataset):\n",
        "  def __init__(self,sentences,labels, tokenizer,max_len=64) -> None:\n",
        "    self.sentences = sentences\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  def __len__(self):\n",
        "    return len(self.sentences)\n",
        "  def __getitem__(self,idx):\n",
        "    words = self.sentences[idx]\n",
        "    lbls = self.labels[idx]\n",
        "    encoding = self.tokenizer(\n",
        "        words,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=self.max_len,\n",
        "        is_split_into_words=True\n",
        "    )\n",
        "    word_ids = encoding.word_ids(batch_index = 0)\n",
        "    label_ids = []\n",
        "    for w in word_ids:\n",
        "      if w is None:\n",
        "        label_ids.append(-100)\n",
        "      else:\n",
        "        label_ids.append(label2id[lbls[w]])\n",
        "    return {\n",
        "        'input_ids' : encoding['input_ids'].squeeze(),\n",
        "        'attention_mask' : encoding['attention_mask'].squeeze(),\n",
        "        'labels' : torch.tensor(label_ids)\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCRopgGZD_-K"
      },
      "outputs": [],
      "source": [
        "# DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataset = NerDataSet(train_sentences,train_labels,tokenizer,max_len=10)\n",
        "train_loader = DataLoader(train_dataset,batch_size=2,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3j1dycAE1MP"
      },
      "outputs": [],
      "source": [
        "# 모델 선언 및 학습\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "model = SimpleNERModel(num_labels=len(label_list))\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcI83K0GFv0m",
        "outputId": "71b32f94-b685-4f05-f056-bbdb16f67457"
      },
      "outputs": [],
      "source": [
        "for epoch in range(10):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for batch in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    logits = model(input_ids, attention_mask)\n",
        "    loss = criterion(logits.view(-1, model.num_labels), labels.view(-1))\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(f'epoch {epoch+1} loss : {total_loss/len(train_loader):.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yon6E-oKdAS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h2-guVTI2nU",
        "outputId": "8572529d-1ed1-4cd0-9885-6e4c38a53198"
      },
      "outputs": [],
      "source": [
        "sample_sentence, sample_label =  train_sentences[0], train_labels[0]\n",
        "print(sample_sentence, sample_label)\n",
        "encoding = tokenizer(\n",
        "        sample_sentence,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=4,\n",
        "        is_split_into_words=True\n",
        "    )\n",
        "print(encoding)\n",
        "input_ids = encoding['input_ids'].to(device)\n",
        "attention_mask = encoding['attention_mask'].to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  logits = model(input_ids, attention_mask) \n",
        "  predictions = torch.argmax(logits, dim=-1)[0]\n",
        "print('결과')\n",
        "word_ids = encoding.word_ids(batch_index=0)\n",
        "for i ,word_idx in enumerate(word_ids):\n",
        "  if word_idx is not None:\n",
        "    pred_label = id2label[predictions[i].item()]\n",
        "    print(f'{sample_sentence[word_idx]} -> {pred_label} 정답 : {sample_label[word_idx]}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
